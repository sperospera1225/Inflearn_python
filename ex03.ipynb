{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data = soup.find('p', class_='cssstyle')\n",
    "- data = soup.find('p', 'cssstyle')\n",
    "- data = soup.find('p', attrs = {'align': 'center'})\n",
    "- data = soup.find(id='body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "황경석\n",
      "더필름\n",
      "박현선\n",
      "싱어송라이터\n",
      "하선호\n",
      "레이블대표\n",
      "하태경\n",
      "양준혁 여자친구나이\n",
      "컨트롤러\n",
      "노지훈\n",
      "월북\n",
      "박지원\n",
      "레이블\n",
      "박지원 청문회\n",
      "양준혁\n",
      "김병기\n",
      "이언주 헌법소원\n",
      "야생마 주작\n",
      "성준\n",
      "부산 비\n"
     ]
    }
   ],
   "source": [
    "# 네이버 실시간 검색어 순위 크롤링\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# 유저 설정\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36'}\n",
    "res = requests.get('https://datalab.naver.com/keyword/realtimeList.naver?where=main', headers = headers)\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "# span.item_title 정보 선택\n",
    "mydata = soup.select('span.item_title')\n",
    "# for문 출력\n",
    "for item in mydata:\n",
    "    print(item.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 크롤링 후처리\n",
    "* \\n, 불필요한 데이터 삭제, 깔끔하게 문자열 정리  ->  파이썬 문자열 관련함수 사용해서 처리함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 강사가 실제 사용하는 자동 프로그램 소개\n",
      "2. 필요한 프로그램 설치 시연\n",
      "3. 데이터를 엑셀 파일로 만들기\n",
      "4. 엑셀 파일 이쁘게! 이쁘게!\n",
      "5. 나대신 주기적으로 파이썬 프로그램 실행하기\n",
      "6. 파이썬으로 슬랙(slack) 메신저에 글쓰기\n",
      "7. 웹사이트 변경사항 주기적으로 체크해서, 메신저로 알람주기\n",
      "8. 네이버 API 사용해서, 블로그에 글쓰기\n",
      "9. 자동으로 쿠팡파트너스 API 로 가져온 상품 정보, 네이버 블로그/트위터에 홍보하기\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('https://davelee-fun.github.io/blog/crawl_test')\n",
    "soup = BeautifulSoup(res.content,'html.parser')\n",
    "section = soup.find('ul',id='dev_course_list')\n",
    "titles = section.find_all('li','course')\n",
    "for index, title in enumerate(titles):\n",
    "    print(str(index + 1)+\".\",title.get_text().split('[')[0].split('-')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 일정, 커리큘럼 타이틀, 난이도\n",
      " 5.1 ~ 6.15, 나만의 엣지있는 블로그 사이트 만들기 (취미로 익히는 IT), 초급\n",
      " 6.16 ~ 7.31, 파이썬과 데이터과학 첫걸음 (IT 기본기 익히기), 중급\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분석시 그냥 td만 추출해서 한번에 해버리면 하나의 데이터가 잘못되었을시 \n",
    "# 전체순서가 꼬일우려가 있으므로 tr을 추출한다음 td를 for문을 호출해서 다시 추출한다\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://davelee-fun.github.io/blog/crawl_html_css.html')\n",
    "soup = BeautifulSoup(res.content,'html.parser')\n",
    "items = soup.select('tr')\n",
    "for item in items:\n",
    "    columns = item.select('td')\n",
    "    row_str = ' '\n",
    "    for column in columns:\n",
    "        row_str += ', ' + column.get_text()\n",
    "    print(row_str[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 비말차단마스크\n",
      "2. 헤드셋\n",
      "3. 면도기\n",
      "4. 에어팟 프로\n",
      "5. KF94 마스크\n",
      "6. 키보드\n",
      "7. 이어폰\n",
      "8. 스피커\n",
      "9. 마우스\n",
      "10. 마스크\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = requests.get('https://search.shopping.naver.com/best100v2/main.nhn')\n",
    "soup = BeautifulSoup(html.content, 'html.parser')\n",
    "items = soup.select('ul#popular_srch_lst > li > span.txt')\n",
    "for index, item in enumerate(items):\n",
    "    print(str(index + 1)+\".\",item.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 트리플에이 무지티 반팔티 무지 반팔티셔츠...\n",
      "2. 폴로랄프로렌 포니로고 남녀공용 반팔티\n",
      "3. SOUP 기본 점프슈트 SW6OV90\n",
      "4. 나이키 스포츠웨어 우븐 쇼츠 멘즈 검흰 데...\n",
      "5. 빅사이즈 무지 부드러운 20수 오버핏 반팔티...\n",
      "6. 시원한 아이스냉장고 사방스판슬렉스여름밴...\n",
      "7. 남자 여름 전체스판 빈티지 린넨 세미와이드...\n",
      "8. 타미힐피거 플래그로고 남녀공용 반팔티\n",
      "9. 오조이 1+1 플리츠 주름 찰랑찰랑 세상편한...\n",
      "10. 지오다노 테이퍼드 링클프리 팬츠 110505\n",
      "11. 지오다노 김영광 반팔 피케 티셔츠 010501\n",
      "12. 남자 7부 오버핏 박스 여름 면 반팔 CAMP 커...\n",
      "13. 16컬러 링클프리 캐주얼 세미오버핏 린넨와...\n",
      "14. 방탄꼭지 스킨엔진 프리미엄 3세대 남자 니...\n",
      "15. 라코스테 여성 미니 피케 원피스 EF8470\n",
      "16. 남자 무봉제 심리스 팬티 드로즈\n",
      "17. 오조이 1+1 플리츠 찰랑찰랑 세상편한 조거...\n",
      "18. 폴로랄프로렌 여성 기본 라운드넥 반팔 티셔...\n",
      "19. 린넨팬츠 스판 9부 마 세미와이드 봄여름 면...\n",
      "20. 아디다스 클라이마쿨 드로즈 5종 세트 7MYD...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = requests.get('https://search.shopping.naver.com/best100v2/detail.nhn?catId=50000000')\n",
    "soup = BeautifulSoup(html.content, 'html.parser')\n",
    "items = soup.select('#productListArea > ul > li > p.cont')\n",
    "# 1위 부터 20위까지만 출력\n",
    "for i in range(0,20):\n",
    "    print(str(i + 1)+\".\",items[i].get_text().strip())\n",
    "# 전체 순위 출력\n",
    "# for index, item in enumerate(items):\n",
    "#     print(str(index + 1)+\".\",item.get_text().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. SK바이오팜\n",
      "2. 삼성전자\n",
      "3. 삼성중공우\n",
      "4. 신도기연\n",
      "5. 카카오\n",
      "6. 셀트리온\n",
      "7. 디피씨\n",
      "8. 대웅제약\n",
      "9. 삼성전자우\n",
      "10. 씨젠\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = requests.get('https://finance.naver.com/sise/')\n",
    "soup = BeautifulSoup(html.content, 'html.parser')\n",
    "items = soup.select('#popularItemList > li > a')\n",
    "for index, item in enumerate(items):\n",
    "    print(str(index + 1)+\".\",item.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 다우산업 25,827.36 상승\n",
      "2. 나스닥 10,207.63 상승\n",
      "3. 홍콩H 10,762.85 상승\n",
      "4. 상해종합 3,332.88 상승\n",
      "5. 니케이225 22,714.44 상승\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = requests.get('https://finance.naver.com/sise/')\n",
    "soup = BeautifulSoup(html.content, 'html.parser')\n",
    "items = soup.select('div.rgt > ul.lst_major > li')\n",
    "for index, item in enumerate(items):\n",
    "    item1 = item.select_one('a')\n",
    "    item2 = item.select_one('span') # 상승한 종목만 찾을려면 span.up\n",
    "    item3 = item.select_one('span.blind')\n",
    "    print(str(index + 1)+\".\",item1.get_text().strip(),item2.get_text().strip(),item3.get_text().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
